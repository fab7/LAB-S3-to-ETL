{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to create an AWS Redshift cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set verbosity to 0|1|2 (default=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBOSE = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP-1:  Load cluster parameters from `myDWH.cfg`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Param       Value\n",
      "0         CLUSTER_TYPE  multi-node\n",
      "1   CLUSTER_NODE_COUNT           2\n",
      "2    CLUSTER_NODE_TYPE   dc2.large\n",
      "3         CLUSTER_NAME  dwhCluster\n",
      "4      CLUSTER_DB_NAME         dwh\n",
      "5      CLUSTER_DB_USER     dwhuser\n",
      "6  CLUSTER_DB_PASSWORD    Passw0rd\n",
      "7      CLUSTER_DB_PORT        5439\n",
      "8    AWS_RESOURCE_NAME     dwhRole\n",
      "9           AWS_REGION   us-west-2\n"
     ]
    }
   ],
   "source": [
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('../myDWH.cfg'))\n",
    "\n",
    "# Retrieve the USER-related  parameters\n",
    "USR_KEY                = config.get('USR','USR_KEY')\n",
    "USR_SECRET             = config.get('USR','USR_SECRET')\n",
    "\n",
    "# Retrieve the AWS-related  parameters\n",
    "AWS_REGION             = config.get('AWS','AWS_REGION')\n",
    "\n",
    "# Retrieve the CLUSTER-related  parameters\n",
    "CLUSTER_NAME           = config.get(\"CLUSTER\",\"CLUSTER_NAME\")\n",
    "CLUSTER_TYPE           = config.get(\"CLUSTER\",\"CLUSTER_TYPE\")\n",
    "CLUSTER_NODE_TYPE      = config.get(\"CLUSTER\",\"CLUSTER_NODE_TYPE\")\n",
    "CLUSTER_NODE_COUNT     = config.get(\"CLUSTER\",\"CLUSTER_NODE_COUNT\")\n",
    "\n",
    "CLUSTER_DB_NAME        = config.get(\"CLUSTER\",\"CLUSTER_DB_NAME\")\n",
    "CLUSTER_DB_USER        = config.get(\"CLUSTER\",\"CLUSTER_DB_USER\")\n",
    "CLUSTER_DB_PASSWORD    = config.get(\"CLUSTER\",\"CLUSTER_DB_PASSWORD\")\n",
    "CLUSTER_DB_PORT        = config.get(\"CLUSTER\",\"CLUSTER_DB_PORT\")\n",
    "\n",
    "# Retrieve the IAM-related parameters\n",
    "AWS_RESOURCE_NAME      = config.get(\"IAM_ROLE\", \"AWS_RESOURCE_NAME\")\n",
    "\n",
    "(CLUSTER_DB_USER, CLUSTER_DB_PASSWORD, CLUSTER_DB_NAME)\n",
    "\n",
    "if VERBOSE > 0:\n",
    "    df = pd.DataFrame({\"Param\":\n",
    "                       [\"CLUSTER_TYPE\", \"CLUSTER_NODE_COUNT\", \"CLUSTER_NODE_TYPE\", \"CLUSTER_NAME\", \"CLUSTER_DB_NAME\", \"CLUSTER_DB_USER\", \"CLUSTER_DB_PASSWORD\", \"CLUSTER_DB_PORT\", \"AWS_RESOURCE_NAME\", \"AWS_REGION\"],\n",
    "                       \"Value\":\n",
    "                       [ CLUSTER_TYPE ,  CLUSTER_NODE_COUNT ,  CLUSTER_NODE_TYPE ,  CLUSTER_NAME ,  CLUSTER_DB_NAME ,  CLUSTER_DB_USER ,  CLUSTER_DB_PASSWORD ,  CLUSTER_DB_PORT ,  AWS_RESOURCE_NAME ,  AWS_REGION ]\n",
    "                      })\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP-2: Create clients for EC2, S3, IAM, and Redshift   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create clients for EC2, S3, IAM, and Redshift\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "# Note-1: To use Boto3, we must indicate which services we are going to use.\n",
    "#         In our case it will be: ec2, s3, iam and redshift.\n",
    "\n",
    "# Note-2: If you have the AWS CLI installed, then you can use the AWS configure\n",
    "#         command to configure the credentials file instead of passing them as parameters.\n",
    "\n",
    "if VERBOSE > 0:\n",
    "    print(\"Create clients for EC2, S3, IAM, and Redshift\")\n",
    "\n",
    "ec2 = boto3.resource('ec2',\n",
    "                       region_name=AWS_REGION,\n",
    "                       aws_access_key_id=USR_KEY,\n",
    "                       aws_secret_access_key=USR_SECRET\n",
    "                    )\n",
    "\n",
    "s3 = boto3.resource('s3',\n",
    "                       region_name=AWS_REGION,\n",
    "                       aws_access_key_id=USR_KEY,\n",
    "                       aws_secret_access_key=USR_SECRET\n",
    "                   )\n",
    "\n",
    "iam = boto3.client('iam',aws_access_key_id=USR_KEY,\n",
    "                     aws_secret_access_key=USR_SECRET,\n",
    "                     region_name=AWS_REGION\n",
    "                  )\n",
    "\n",
    "redshift = boto3.client('redshift',\n",
    "                       region_name=AWS_REGION,\n",
    "                       aws_access_key_id=USR_KEY,\n",
    "                       aws_secret_access_key=USR_SECRET\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## STEP-3: Create an IAM Role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating IAM Role\n",
      "\n",
      "WARNING: A role with similar name most likely already exists.\n",
      "An error occurred (EntityAlreadyExists) when calling the CreateRole operation: Role with name dwhRole already exists.\n",
      "Attaching Policy - Allow access to all Amazon S3 buckets (ReadOnly)\n",
      "Response: 200\n",
      "\n",
      "Getting the IAM role ARN\n",
      "arn:aws:iam::449575054145:role/dwhRole\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from botocore.exceptions import ClientError\n",
    "\n",
    "### STEP-3.1: CREATE IAM ROLE #############################\n",
    "if VERBOSE > 0:\n",
    "    print(\"Creating IAM Role\")\n",
    "    \n",
    "try:\n",
    "    dwhRole = iam.create_role(\n",
    "        Path='/',\n",
    "        RoleName=AWS_RESOURCE_NAME,\n",
    "        Description = \"Allows Redshift clusters to call AWS services on your behalf.\",\n",
    "        AssumeRolePolicyDocument=json.dumps(\n",
    "            {'Statement': [{'Action': 'sts:AssumeRole',\n",
    "               'Effect': 'Allow',\n",
    "               'Principal': {'Service': 'redshift.amazonaws.com'}}],\n",
    "               'Version': '2012-10-17'})\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(\"\\nWARNING: A role with similar name most likely already exists.\")\n",
    "    if VERBOSE > 0:\n",
    "        print(e)\n",
    "else:\n",
    "    if VERBOSE > 0:\n",
    "        print(\"\\tFYI - Details of the current role with name \\'%s\\':\" % dwhRole['Role']['RoleName'])\n",
    "        print(dwhRole)\n",
    "        print()\n",
    "\n",
    "### STEP-3.2: ATTACH POLICY ###############################\n",
    "response = iam.attach_role_policy(RoleName=AWS_RESOURCE_NAME, \n",
    "                                  PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\"\n",
    "                                 )['ResponseMetadata']['HTTPStatusCode']\n",
    "if VERBOSE > 0:\n",
    "    print(\"Attaching Policy - Allow access to all Amazon S3 buckets (ReadOnly)\") \n",
    "    # See also - https://docs.aws.amazon.com/redshift/latest/dg/c-getting-started-using-spectrum-create-role.html\n",
    "    print('Response: %i\\n' % response)\n",
    "\n",
    "### STEP-3.3: RETRIEVE THE IAM ROLE ARN ###################\n",
    "roleArn = iam.get_role(RoleName=AWS_RESOURCE_NAME)['Role']['Arn']\n",
    "if VERBOSE > 0:\n",
    "    print('Getting the IAM role ARN')\n",
    "    print(roleArn + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP-4:  Create the Redshift cluster\n",
    "- Create a [RedShift Cluster](https://console.aws.amazon.com/redshiftv2/home)\n",
    "- For complete arguments to `create_cluster`, see [docs](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/redshift.html#Redshift.Client.create_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Redshift cluster\n",
      "An error occurred (ClusterAlreadyExists) when calling the CreateCluster operation: Cluster already exists\n"
     ]
    }
   ],
   "source": [
    "if VERBOSE > 0:\n",
    "    print(\"Creating Redshift cluster\")\n",
    "\n",
    "try:\n",
    "    response = redshift.create_cluster(\n",
    "        # Parameters for HW\n",
    "        ClusterType   = CLUSTER_TYPE,\n",
    "        NodeType      = CLUSTER_NODE_TYPE,\n",
    "        NumberOfNodes = int(CLUSTER_NODE_COUNT),\n",
    "        # Parameters for Identifiers & Credentials\n",
    "        #  - DBName (string) -\n",
    "        #      The name of the first database to be created when the cluster is created.\n",
    "        #  - ClusterIdentifier (string) –\n",
    "        #      A unique identifier for the cluster. You use this identifier to refer to\n",
    "        #      the cluster for any subsequent cluster operations such as deleting or modifying.\n",
    "        #  - MasterUsername (string) –\n",
    "        #      The user name associated with the admin user for the cluster that is being created.\n",
    "        DBName             = CLUSTER_DB_NAME,\n",
    "        ClusterIdentifier  = CLUSTER_NAME,\n",
    "        MasterUsername     = CLUSTER_DB_USER,\n",
    "        MasterUserPassword = CLUSTER_DB_PASSWORD,       \n",
    "        # Parameters for Roles (to allow s3 access)\n",
    "        #   - IamRoles (list) – \n",
    "        #      A list of IAM roles that can be used by the cluster to access other AWS services.\n",
    "        #      You must supply the IAM roles in their Amazon Resource Name (ARN) format.\n",
    "        IamRoles=[roleArn]\n",
    "    )\n",
    "    if VERBOSE > 0:\n",
    "        print(response)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP-5:  Wait for the cluster to become available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster is available\n",
      "\n",
      "FYI: This is the created cluster ENDPOINT and cluster Role ARN\n",
      "\tCLUSTER_ENDPOINT ::  dwhcluster.c4p6b3uqdbp8.us-west-2.redshift.amazonaws.com\n",
      "\tCLUSTER_ROLE_ARN ::  arn:aws:iam::449575054145:role/dwhRole\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def prettyRedshiftProps(props):\n",
    "    pd.set_option('display.max_colwidth', -1)\n",
    "    keysToShow = [\"ClusterIdentifier\", \"NodeType\", \"ClusterStatus\", \"MasterUsername\", \"DBName\", \"Endpoint\", \"NumberOfNodes\", 'VpcId']\n",
    "    x = [(k, v) for k,v in props.items() if k in keysToShow]\n",
    "    return pd.DataFrame(data=x, columns=[\"Key\", \"Value\"])\n",
    "\n",
    "while redshift.describe_clusters(ClusterIdentifier=CLUSTER_NAME)['Clusters'][0]['ClusterStatus'] != 'available':\n",
    "    if VERBOSE > 0:\n",
    "        print(\"Waiting for cluster to come up...\")\n",
    "    time.sleep(5)\n",
    "print('Cluster is available')\n",
    "\n",
    "# Describe the created cluster\n",
    "cluster_props = redshift.describe_clusters(ClusterIdentifier=CLUSTER_NAME)['Clusters'][0]\n",
    "\n",
    "if VERBOSE > 0:\n",
    "    prettyRedshiftProps(cluster_props)\n",
    "\n",
    "# Display the cluster endpoint and role ARN \n",
    "CLUSTER_ENDPOINT = cluster_props['Endpoint']['Address']\n",
    "CLUSTER_ROLE_ARN = cluster_props['IamRoles'][0]['IamRoleArn']\n",
    "print(\"\\nFYI: This is the created cluster ENDPOINT and cluster Role ARN\")\n",
    "print(\"\\tCLUSTER_ENDPOINT :: \", CLUSTER_ENDPOINT)\n",
    "print(\"\\tCLUSTER_ROLE_ARN :: \", CLUSTER_ROLE_ARN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP-6: Open a TCP port to access the cluster endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening a TCP port to the endpoint\n",
      "Default security group: ec2.SecurityGroup(id='sg-001ef50f88654cbd0')\n",
      "An error occurred (InvalidPermission.Duplicate) when calling the AuthorizeSecurityGroupIngress operation: the specified rule \"peer: 0.0.0.0/0, TCP, from port: 5439, to port: 5439, ALLOW\" already exists\n"
     ]
    }
   ],
   "source": [
    "if VERBOSE > 0:\n",
    "    print(\"Opening a TCP port to the endpoint\")\n",
    "\n",
    "vpc = ec2.Vpc(id=cluster_props['VpcId'])\n",
    "    \n",
    "# Get the list of security groups.\n",
    "#  - A security group acts as a firewall for the traffic to and from the resources in your VPC.\n",
    "default_sg = list(vpc.security_groups.all())[0]\n",
    "if VERBOSE > 0:\n",
    "    print(\"Default security group: %s\" % default_sg)    \n",
    "    \n",
    "try:\n",
    "    default_sg.authorize_ingress(\n",
    "        GroupName=default_sg.group_name,\n",
    "        CidrIp='0.0.0.0/0',\n",
    "        IpProtocol='TCP',\n",
    "        FromPort=int(CLUSTER_DB_PORT),\n",
    "        ToPort=int(CLUSTER_DB_PORT)\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP-7: Make sure you can connect to the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postgresql://dwhuser:Passw0rd@dwhcluster.c4p6b3uqdbp8.us-west-2.redshift.amazonaws.com:5439/dwh\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Connected: dwhuser@dwh'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn_string=\"postgresql://{}:{}@{}:{}/{}\".format(CLUSTER_DB_USER, CLUSTER_DB_PASSWORD, CLUSTER_ENDPOINT, CLUSTER_DB_PORT, CLUSTER_DB_NAME)\n",
    "print(conn_string)\n",
    "%sql $conn_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ===== TEST-AND-DEBUG-SECTION ===== "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try to check out our own bucket on S3 (\"fab-se4s-bucket\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3.ObjectSummary(bucket_name='fab-se4s-bucket', key='fab-cartoon.jpg')\n"
     ]
    }
   ],
   "source": [
    "mySe4sBucket = s3.Bucket(\"fab-se4s-bucket\")\n",
    "\n",
    "# Iterate over bucket objects starting with \"ssbgz\" and print\n",
    "for obj in mySe4sBucket.objects.all():\n",
    "    print(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's take a look at the `song_data` set on S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3.ObjectSummary(bucket_name='udacity-dend', key='song_data/A/A/A/TRAAAAK128F9318786.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='song_data/A/A/A/TRAAAAV128F421A322.json')\n"
     ]
    }
   ],
   "source": [
    "udacity_bucket = s3.Bucket(\"udacity-dend\")\n",
    "\n",
    "# Iterate over bucket objects starting with \"song_data/A/A\" and print\n",
    "for obj in udacity_bucket.objects.filter(Prefix=\"song_data/A/A/A/TRAAAA\"):\n",
    "    print(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's take a look at the log_data set on S3¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3.ObjectSummary(bucket_name='udacity-dend', key='log_data/2018/11/2018-11-12-events.json')\n"
     ]
    }
   ],
   "source": [
    "udacity_bucket = s3.Bucket(\"udacity-dend\")\n",
    "\n",
    "# Iterate over bucket objects starting with \"log_data\" and print\n",
    "for obj in udacity_bucket.objects.filter(Prefix=\"log_data/2018/11/2018-11-12\"):\n",
    "    print(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try to run a COPY query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= LOADING TABLE: ** staging_events ** =======\n",
      "\n",
      "COPY staging_events FROM 's3://udacity-dend/log_data'\n",
      "    credentials 'aws_iam_role=arn:aws:iam::449575054145:role/dwhRole'\n",
      "    region 'us-west-2'\n",
      "    TIMEFORMAT AS 'epochmillisecs' \n",
      "    JSON 's3://udacity-dend/log_json_path.json';\n",
      "\n",
      " * postgresql://dwhuser:***@dwhcluster.c4p6b3uqdbp8.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "table = \"staging_events\"\n",
    "SQL_COPY = \"\"\"\n",
    "COPY {} FROM 's3://udacity-dend/log_data'\n",
    "    credentials 'aws_iam_role=arn:aws:iam::449575054145:role/dwhRole'\n",
    "    region 'us-west-2'\n",
    "    TIMEFORMAT AS 'epochmillisecs' \n",
    "    JSON 's3://udacity-dend/log_json_path.json';\n",
    "\"\"\".format(table)\n",
    "\n",
    "print(\"======= LOADING TABLE: ** {} ** =======\".format(table))\n",
    "print(SQL_COPY)\n",
    "\n",
    "t0 = time()\n",
    "%sql $SQL_COPY\n",
    "loadTime = time()-t0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dimSong: Let's try to LOAD  the table by INSERTING new data from the staging tables while handling duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwhcluster.c4p6b3uqdbp8.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "1 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>song_id</th>\n",
       "        <th>title</th>\n",
       "        <th>artist_id</th>\n",
       "        <th>year</th>\n",
       "        <th>duration</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>SOHOZBI12A8C132E3C</td>\n",
       "        <td>Smash It Up</td>\n",
       "        <td>AR0MWD61187B9B2B12</td>\n",
       "        <td>2000</td>\n",
       "        <td>195</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[('SOHOZBI12A8C132E3C', 'Smash It Up', 'AR0MWD61187B9B2B12', 2000, Decimal('195'))]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "    SELECT song_id, title, artist_id, year, duration FROM staging_songs WHERE year=2000;\n",
    "\"\"\"\n",
    "%sql $query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwhcluster.c4p6b3uqdbp8.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "5 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>artist_id</th>\n",
       "        <th>artist_latitude</th>\n",
       "        <th>artist_location</th>\n",
       "        <th>artist_longitude</th>\n",
       "        <th>artist_name</th>\n",
       "        <th>duration</th>\n",
       "        <th>num_songs</th>\n",
       "        <th>song_id</th>\n",
       "        <th>title</th>\n",
       "        <th>year</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>ARJNIUY12298900C91</td>\n",
       "        <td>None</td>\n",
       "        <td></td>\n",
       "        <td>None</td>\n",
       "        <td>Adelitas Way</td>\n",
       "        <td>213</td>\n",
       "        <td>1</td>\n",
       "        <td>SOBLFFE12AF72AA5BA</td>\n",
       "        <td>Scream</td>\n",
       "        <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>ARJNIUY12298900C91</td>\n",
       "        <td>None</td>\n",
       "        <td></td>\n",
       "        <td>None</td>\n",
       "        <td>Adelitas Way</td>\n",
       "        <td>213</td>\n",
       "        <td>1</td>\n",
       "        <td>SOBLFFE12AF72AA5BA</td>\n",
       "        <td>Scream</td>\n",
       "        <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>ARJNIUY12298900C91</td>\n",
       "        <td>None</td>\n",
       "        <td></td>\n",
       "        <td>None</td>\n",
       "        <td>Adelitas Way</td>\n",
       "        <td>213</td>\n",
       "        <td>1</td>\n",
       "        <td>SOBLFFE12AF72AA5BA</td>\n",
       "        <td>Scream</td>\n",
       "        <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>ARJNIUY12298900C91</td>\n",
       "        <td>None</td>\n",
       "        <td></td>\n",
       "        <td>None</td>\n",
       "        <td>Adelitas Way</td>\n",
       "        <td>213</td>\n",
       "        <td>1</td>\n",
       "        <td>SOBLFFE12AF72AA5BA</td>\n",
       "        <td>Scream</td>\n",
       "        <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>ARJNIUY12298900C91</td>\n",
       "        <td>None</td>\n",
       "        <td></td>\n",
       "        <td>None</td>\n",
       "        <td>Adelitas Way</td>\n",
       "        <td>213</td>\n",
       "        <td>1</td>\n",
       "        <td>SOBLFFE12AF72AA5BA</td>\n",
       "        <td>Scream</td>\n",
       "        <td>2009</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[('ARJNIUY12298900C91', None, '', None, 'Adelitas Way', Decimal('213'), 1, 'SOBLFFE12AF72AA5BA', 'Scream', 2009),\n",
       " ('ARJNIUY12298900C91', None, '', None, 'Adelitas Way', Decimal('213'), 1, 'SOBLFFE12AF72AA5BA', 'Scream', 2009),\n",
       " ('ARJNIUY12298900C91', None, '', None, 'Adelitas Way', Decimal('213'), 1, 'SOBLFFE12AF72AA5BA', 'Scream', 2009),\n",
       " ('ARJNIUY12298900C91', None, '', None, 'Adelitas Way', Decimal('213'), 1, 'SOBLFFE12AF72AA5BA', 'Scream', 2009),\n",
       " ('ARJNIUY12298900C91', None, '', None, 'Adelitas Way', Decimal('213'), 1, 'SOBLFFE12AF72AA5BA', 'Scream', 2009)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"SELECT * FROM staging_songs WHERE year=2009; \"\"\"\n",
    "%sql $query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwhcluster.c4p6b3uqdbp8.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "48 rows affected.\n",
      " * postgresql://dwhuser:***@dwhcluster.c4p6b3uqdbp8.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "48 rows affected.\n",
      " * postgresql://dwhuser:***@dwhcluster.c4p6b3uqdbp8.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "24 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STEP-1: Delete *ALL* the rows in the table before inserting again\n",
    "#-------------------------------------------------------------------\n",
    "delete_raws = \"\"\"DELETE FROM dimSong;\"\"\"\n",
    "%sql $delete_raws\n",
    "\n",
    "# STEP-2: Load the 'dimSong' table by inserting new data from the \n",
    "#     staging tables while handling duplicates.\n",
    "#-------------------------------------------------------------------\n",
    "# insert_with_duplicates = \"\"\"\n",
    "#     INSERT INTO dimSong (song_id, title, artist_id, year, duration)\n",
    "#     SELECT song_id, title, artist_id, year, duration\n",
    "#     FROM staging_songs;\n",
    "# \"\"\"\n",
    "# %sql $insert_with_duplicates\n",
    "\n",
    "rank_the_duplicates = \"\"\"\n",
    "    (SELECT\n",
    "        song_id, \n",
    "        title, \n",
    "        artist_id, \n",
    "        year, \n",
    "        duration, \n",
    "        ROW_NUMBER() OVER (PARTITION BY song_id) AS song_id_ranked \n",
    "    FROM staging_songs);\n",
    "\"\"\"\n",
    "%sql $rank_the_duplicates\n",
    "\n",
    "insert_wo_duplicates = \"\"\"\n",
    "    INSERT INTO dimSong (song_id, title, artist_id, year, duration)\n",
    "    SELECT song_id, title, artist_id, year, duration \n",
    "    FROM\n",
    "        (SELECT\n",
    "            song_id, \n",
    "            title, \n",
    "            artist_id, \n",
    "            year, \n",
    "            duration, \n",
    "            ROW_NUMBER() OVER (PARTITION BY song_id) AS song_id_ranked \n",
    "        FROM staging_songs)\n",
    "    WHERE song_id_ranked = 1;\n",
    "\"\"\"\n",
    "%sql $insert_wo_duplicates\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwhcluster.c4p6b3uqdbp8.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "24 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>song_id</th>\n",
       "        <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>SOEKAZG12AB018837E</td>\n",
       "        <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>SOAPERH12A58A787DC</td>\n",
       "        <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>SOSMJFC12A8C13DE0C</td>\n",
       "        <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>SOBRKGM12A8C139EF6</td>\n",
       "        <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>SOAFBCP12A8C13CC7D</td>\n",
       "        <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>SOCIWDW12A8C13D406</td>\n",
       "        <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>SOBLFFE12AF72AA5BA</td>\n",
       "        <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>SODZYPO12A8C13A91E</td>\n",
       "        <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>SOIGICF12A8C141BC5</td>\n",
       "        <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>SOERIDA12A6D4F8506</td>\n",
       "        <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>SONQPZK12AB0182D84</td>\n",
       "        <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>SOFRDWL12A58A7CEF7</td>\n",
       "        <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>SONRWUU12AF72A4283</td>\n",
       "        <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>SOKTJDS12AF72A25E5</td>\n",
       "        <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>SOXZYWX12A6310ED0C</td>\n",
       "        <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>SOTAZDY12AB0187616</td>\n",
       "        <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>SOABWAP12A8C13F82A</td>\n",
       "        <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>SOFSOCN12A8C143F5D</td>\n",
       "        <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>SOHKNRJ12A6701D1F8</td>\n",
       "        <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>SOHOZBI12A8C132E3C</td>\n",
       "        <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>SOIGHOD12A8C13B5A1</td>\n",
       "        <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>SOOVHYF12A8C134892</td>\n",
       "        <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>SOQPWCR12A6D4FB2A3</td>\n",
       "        <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>SORRNOC12AB017F52B</td>\n",
       "        <td>5</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[('SOEKAZG12AB018837E', 5),\n",
       " ('SOAPERH12A58A787DC', 5),\n",
       " ('SOSMJFC12A8C13DE0C', 5),\n",
       " ('SOBRKGM12A8C139EF6', 5),\n",
       " ('SOAFBCP12A8C13CC7D', 5),\n",
       " ('SOCIWDW12A8C13D406', 5),\n",
       " ('SOBLFFE12AF72AA5BA', 5),\n",
       " ('SODZYPO12A8C13A91E', 5),\n",
       " ('SOIGICF12A8C141BC5', 5),\n",
       " ('SOERIDA12A6D4F8506', 5),\n",
       " ('SONQPZK12AB0182D84', 5),\n",
       " ('SOFRDWL12A58A7CEF7', 5),\n",
       " ('SONRWUU12AF72A4283', 5),\n",
       " ('SOKTJDS12AF72A25E5', 5),\n",
       " ('SOXZYWX12A6310ED0C', 5),\n",
       " ('SOTAZDY12AB0187616', 5),\n",
       " ('SOABWAP12A8C13F82A', 5),\n",
       " ('SOFSOCN12A8C143F5D', 5),\n",
       " ('SOHKNRJ12A6701D1F8', 5),\n",
       " ('SOHOZBI12A8C132E3C', 5),\n",
       " ('SOIGHOD12A8C13B5A1', 5),\n",
       " ('SOOVHYF12A8C134892', 5),\n",
       " ('SOQPWCR12A6D4FB2A3', 5),\n",
       " ('SORRNOC12AB017F52B', 5)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "    SELECT song_id, COUNT(*) as count \n",
    "    FROM dimsong \n",
    "    GROUP BY song_id ORDER BY count;\n",
    "\"\"\"\n",
    "%sql $query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dimArtist: Let's try to LOAD  the table by INSERTING new data from the staging tables while handling duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query =\"\"\"\n",
    "    INSERT INTO dimArtist (artist_id, artist_name, artist_location,\n",
    "                           artist_latitude, artist_longitude)\n",
    "        SELECT artist_id, artist_name, artist_location, \n",
    "                artist_latitude, artist_longitude\n",
    "        FROM \n",
    "            staging_songs;\n",
    "\"\"\"\n",
    "%sql $query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dimTime: Let's try to LOAD  the table by INSERTING new data from the staging tables while handling duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwhcluster.c4p6b3uqdbp8.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "24168 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "    INSERT INTO dimTime (start_time, hour, day, week, month, year, weekday)\n",
    "        SELECT\n",
    "            ts                         AS start_time,\n",
    "            EXTRACT(hour      FROM ts) AS hour,\n",
    "            EXTRACT(day       FROM ts) AS day,\n",
    "            EXTRACT(week      FROM ts) AS week,\n",
    "            EXTRACT(month     FROM ts) AS month,\n",
    "            EXTRACT(year      FROM ts) AS year,\n",
    "            EXTRACT(dayofweek FROM ts) AS weekday\n",
    "    FROM\n",
    "        staging_events\n",
    "    WHERE ts IS NOT NULL;    \n",
    "\"\"\"\n",
    "\n",
    "%sql $query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dimUser: Let's try to LOAD the table by INSERTING new data from the staging tables while handling duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwhcluster.c4p6b3uqdbp8.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "97 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank_the_duplicates = \"\"\"\n",
    "INSERT INTO dimUser (user_id, first_name, last_name, gender, level)\n",
    "    SELECT\n",
    "        userid    AS user_id,\n",
    "        firstname AS first_name,\n",
    "        lastname  AS last_name,\n",
    "        gender    AS gender,\n",
    "        level     AS level\n",
    "    FROM\n",
    "        (SELECT\n",
    "            userid,\n",
    "            firstname, \n",
    "            lastname, \n",
    "            gender, \n",
    "            level, \n",
    "            ROW_NUMBER() OVER (PARTITION BY userid) AS userid_ranked \n",
    "        FROM \n",
    "            staging_events)\n",
    "    WHERE userid_ranked=1 AND userid IS NOT NULL;\n",
    "\"\"\"\n",
    "%sql $rank_the_duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dimSongpaly: Let's try to LOAD the table by INSERTING new data from the staging tables while handling duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwhcluster.c4p6b3uqdbp8.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "16 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "insert_query = \"\"\"\n",
    "INSERT INTO factSongplay (start_time, user_id, level, song_id, artist_id,\n",
    "                          session_id,  location, user_agent)\n",
    "    SELECT DISTINCT\n",
    "        staging_events.ts        AS start_time,\n",
    "        staging_events.userId    AS user_id,\n",
    "        staging_events.level     AS level,\n",
    "        staging_songs.song_id    AS song_id,\n",
    "        staging_songs.artist_id  As artist_id,\n",
    "        staging_events.sessionId AS session_id,\n",
    "        staging_events.location  AS location,\n",
    "        staging_events.userAgent AS user_agent\n",
    "    FROM\n",
    "        staging_events\n",
    "    JOIN staging_songs\n",
    "        ON staging_events.song   = staging_songs.title AND \n",
    "           staging_events.artist = staging_songs.artist_name        \n",
    "    WHERE\n",
    "        staging_events.page   = 'NextSong'\n",
    "    ;\n",
    "\"\"\"\n",
    "\n",
    "%sql $insert_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EMERGENCY STEPS TO CLEAR THE CLUSTER\n",
    "\n",
    "<b><font color='red'>- - - - -> Please consider using the `delete_cluster.py` instead <- - - - - - <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'redshift' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e96d7ce3fe5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#### CAREFUL!!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#-- Uncomment & run to delete the created resources\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mredshift\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete_cluster\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mClusterIdentifier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCLUSTER_NAME\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mSkipFinalClusterSnapshot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#### CAREFUL!!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'redshift' is not defined"
     ]
    }
   ],
   "source": [
    "#### CAREFUL!!\n",
    "#-- Uncomment & run to delete the created resources\n",
    "redshift.delete_cluster( ClusterIdentifier=CLUSTER_NAME,  SkipFinalClusterSnapshot=True)\n",
    "#### CAREFUL!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Run this block several times until the cluster really deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myClusterProps = redshift.describe_clusters(ClusterIdentifier=CLUSTER_NAME)['Clusters'][0]\n",
    "prettyRedshiftProps(myClusterProps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### CAREFUL!!\n",
    "#-- Uncomment & run to delete the created resources\n",
    "iam.detach_role_policy(RoleName=AWS_RESOURCE_NAME, PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\")\n",
    "iam.delete_role(RoleName=AWS_RESOURCE_NAME)\n",
    "#### CAREFUL!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
